{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_1 = 'Trees protect us from pollution.'.split()\n",
    "doc_2 = 'We make our home furniture from the tree.'.split()\n",
    "doc_3 = 'There are some holy trees that are worshiped by the people.'.split()\n",
    "doc_4 = 'Trees help to stop soil erosion.'.split()\n",
    "doc_5 = 'Trees absorb greenhouse gases as a result it helps to counter climate change.'.split()\n",
    "doc_6 = 'Trees protect us from the high tide in the coastal area.'.split()\n",
    "doc_7 = 'It also helps in water conservation.'.split()\n",
    "doc_8 = 'Trees keep balance in the ecosystem.'.split()\n",
    "doc_9 = 'We all should plant trees near our house.'.split()\n",
    "doc_10 = 'We must take the necessary steps to save trees.'.split()\n",
    "data = [doc_1 ,doc_2 , doc_3, doc_4, doc_5, doc_6, doc_7, doc_8, doc_9, doc_10]\n",
    "query = 'What do trees do?'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datasets import load_dataset\n",
    "#wiki_bio_dataset = load_dataset('wiki_bio', split='test[0:50]')\n",
    "#docs = wiki_bio_dataset['target_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_docs = []\n",
    "#for i in docs:\n",
    "    \n",
    "#    i = i.replace(\"-lrb-\", \"\")\n",
    "#    i = i.replace(\"-rrb-\", \"\")\n",
    "#    i = i.replace(\".\\n\", \"\")\n",
    "#    i = i.replace(\"''\", \"\")\n",
    "#    i = i.replace(\"``\", \"\")\n",
    "#    i = i.replace(\" ,\", \",\")\n",
    "#    i = i.replace(\" '\", \"\")\n",
    "#    new_docs.append(i)\n",
    "#docs[0] = docs[0].replace(\"-lrb-\", \"\")\n",
    "#new_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_term_count(term, document): # count times term appears in the document\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    for word in document:\n",
    "\n",
    "        if term == word:\n",
    "\n",
    "            count = count + 1 \n",
    "\n",
    "        else:\n",
    "            \n",
    "            count = count\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "#document_term_count(data[0], data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_d_words(document): # count all words in document\n",
    "\n",
    "    count_words = len(document)\n",
    "\n",
    "    return count_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(term, document): # calculate term frequency ; if low, then not much relevance in document, if high then high relevance in document\n",
    "\n",
    "    tf = document_term_count(term, document) / num_d_words(document)\n",
    "\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "#term_frequency(data[0], data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_d_words_augment(document): # find the most frequent word and return how many times it occurred\n",
    "\n",
    "    count_all_words = []\n",
    "\n",
    "    for term in document:\n",
    "\n",
    "        count = document.count(term)\n",
    "\n",
    "        count_all_words.append(count)\n",
    "\n",
    "    max_word = max(count_all_words)\n",
    "\n",
    "    return max_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_term_frequency(term, document): # normalization based on comparing our word against the most frequent word in document ; reduces bias towards longer document\n",
    "\n",
    "    return 0.5 + 0.5 * (document_term_count(term, document) / num_d_words_augment(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "#augmented_term_frequency(data[0], data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_relevant_documents(term, document_set): # count number of documents that contain the term\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    for document in document_set:\n",
    "\n",
    "        if term in document:\n",
    "\n",
    "            count = count + 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            count = count\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_documents(document_set): # count total number of documents in document set\n",
    "\n",
    "    count_docs = len(document_set)\n",
    "\n",
    "    return count_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_document_frequency(term, document_set): # calculate idf ; weighs down common words while scaling up rare words; measures how informative t is; higher idf means rare word, lower idf means less common word\n",
    "\n",
    "    idf = 1 + math.log10(num_documents(document_set) / (num_relevant_documents(term, document_set) + 1))\n",
    "\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "#print(inverse_document_frequency(data[0], data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_terms(document_set): # gather all unique words in all documents\n",
    "\n",
    "    unique_terms = []\n",
    "\n",
    "    for document in document_set:\n",
    "\n",
    "        for term in document:\n",
    "\n",
    "            if term not in unique_terms:\n",
    "\n",
    "                unique_terms.append(term)\n",
    "            \n",
    "            else:\n",
    "\n",
    "                pass\n",
    "    \n",
    "    return unique_terms\n",
    "\n",
    "#unique_terms(data)\n",
    "#unique_words = unique_terms(data)\n",
    "#unique_words = dict.fromkeys(unique_words)\n",
    "#unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def tfidf(document_set, query): # calculate tfidf for all words and return a dictionary with (x, y) : #, where x is the document index, y is the index of the word in unique_words\n",
    "        \n",
    "    unique_words = unique_terms(document_set + query)\n",
    "    tfidf_vals = {}\n",
    "    counter = 0\n",
    "    for document in document_set:\n",
    "        \n",
    "        unique_terms_list = []\n",
    "        counter = counter + 1\n",
    "        for term in document:\n",
    "            counter1 = counter - 1\n",
    "            if term in unique_terms_list:\n",
    "                \n",
    "                pass\n",
    "            \n",
    "            else:\n",
    "            \n",
    "                tf = augmented_term_frequency(term, document)\n",
    "                idf = inverse_document_frequency(term, document_set)\n",
    "            \n",
    "                tfidf = tf * idf\n",
    "                term_idx = np.where(np.asarray(unique_words) == term)\n",
    "                term_idx = list(term_idx)\n",
    "                term_idx = int(term_idx[0])\n",
    "                tfidf_vals[counter1, term_idx] = tfidf\n",
    "                unique_terms_list.append(term)\n",
    "                \n",
    "    \n",
    "    return tfidf_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 1.2218487496163564,\n",
       " (0, 1): 1.5228787452803376,\n",
       " (0, 2): 1.5228787452803376,\n",
       " (0, 3): 1.3979400086720375,\n",
       " (0, 4): 1.6989700043360187,\n",
       " (1, 5): 1.3979400086720375,\n",
       " (1, 6): 1.6989700043360187,\n",
       " (1, 7): 1.5228787452803376,\n",
       " (1, 8): 1.6989700043360187,\n",
       " (1, 9): 1.6989700043360187,\n",
       " (1, 3): 1.3979400086720375,\n",
       " (1, 10): 1.2218487496163564,\n",
       " (1, 11): 1.6989700043360187,\n",
       " (2, 12): 1.274227503252014,\n",
       " (2, 13): 1.6989700043360187,\n",
       " (2, 14): 1.274227503252014,\n",
       " (2, 15): 1.274227503252014,\n",
       " (2, 16): 1.1421590589602533,\n",
       " (2, 17): 1.274227503252014,\n",
       " (2, 18): 1.274227503252014,\n",
       " (2, 19): 1.274227503252014,\n",
       " (2, 10): 0.9163865622122673,\n",
       " (2, 20): 1.274227503252014,\n",
       " (3, 0): 1.2218487496163564,\n",
       " (3, 21): 1.6989700043360187,\n",
       " (3, 22): 1.3979400086720375,\n",
       " (3, 23): 1.6989700043360187,\n",
       " (3, 24): 1.6989700043360187,\n",
       " (3, 25): 1.6989700043360187,\n",
       " (4, 0): 1.2218487496163564,\n",
       " (4, 26): 1.6989700043360187,\n",
       " (4, 27): 1.6989700043360187,\n",
       " (4, 28): 1.6989700043360187,\n",
       " (4, 29): 1.6989700043360187,\n",
       " (4, 30): 1.6989700043360187,\n",
       " (4, 31): 1.6989700043360187,\n",
       " (4, 32): 1.6989700043360187,\n",
       " (4, 33): 1.5228787452803376,\n",
       " (4, 22): 1.3979400086720375,\n",
       " (4, 34): 1.6989700043360187,\n",
       " (4, 35): 1.6989700043360187,\n",
       " (4, 36): 1.6989700043360187,\n",
       " (5, 0): 0.9163865622122673,\n",
       " (5, 1): 1.1421590589602533,\n",
       " (5, 2): 1.1421590589602533,\n",
       " (5, 3): 1.0484550065040281,\n",
       " (5, 10): 1.2218487496163564,\n",
       " (5, 37): 1.274227503252014,\n",
       " (5, 38): 1.274227503252014,\n",
       " (5, 39): 1.0484550065040281,\n",
       " (5, 40): 1.274227503252014,\n",
       " (5, 41): 1.274227503252014,\n",
       " (6, 42): 1.6989700043360187,\n",
       " (6, 43): 1.6989700043360187,\n",
       " (6, 33): 1.5228787452803376,\n",
       " (6, 39): 1.3979400086720375,\n",
       " (6, 44): 1.6989700043360187,\n",
       " (6, 45): 1.6989700043360187,\n",
       " (7, 0): 1.2218487496163564,\n",
       " (7, 46): 1.6989700043360187,\n",
       " (7, 47): 1.6989700043360187,\n",
       " (7, 39): 1.3979400086720375,\n",
       " (7, 10): 1.2218487496163564,\n",
       " (7, 48): 1.6989700043360187,\n",
       " (8, 5): 1.3979400086720375,\n",
       " (8, 49): 1.6989700043360187,\n",
       " (8, 50): 1.6989700043360187,\n",
       " (8, 51): 1.6989700043360187,\n",
       " (8, 16): 1.5228787452803376,\n",
       " (8, 52): 1.6989700043360187,\n",
       " (8, 7): 1.5228787452803376,\n",
       " (8, 53): 1.6989700043360187,\n",
       " (9, 5): 1.3979400086720375,\n",
       " (9, 54): 1.6989700043360187,\n",
       " (9, 55): 1.6989700043360187,\n",
       " (9, 10): 1.2218487496163564,\n",
       " (9, 56): 1.6989700043360187,\n",
       " (9, 57): 1.6989700043360187,\n",
       " (9, 22): 1.3979400086720375,\n",
       " (9, 58): 1.6989700043360187,\n",
       " (9, 59): 1.6989700043360187}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf(data, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_on_query(document_set, query):\n",
    "    \n",
    "    unique_words = unique_terms(document_set + query)\n",
    "    \n",
    "    tfidf_vals = {}\n",
    "    counter = 0\n",
    "    for document in query:\n",
    "        \n",
    "        unique_terms_list = []\n",
    "        \n",
    "        for term in document:\n",
    "            \n",
    "            if term in unique_terms_list:\n",
    "                \n",
    "                pass\n",
    "            \n",
    "            else:\n",
    "            \n",
    "                tf = augmented_term_frequency(term, document)\n",
    "                idf = inverse_document_frequency(term, document_set)\n",
    "            \n",
    "                tfidf = tf * idf\n",
    "                term_idx = np.where(np.asarray(unique_words) == term)\n",
    "                term_idx = list(term_idx)\n",
    "                term_idx = int(term_idx[0])\n",
    "                tfidf_vals[term_idx] = tfidf\n",
    "                unique_terms_list.append(term)\n",
    "                \n",
    "    \n",
    "    return tfidf_vals  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{60: 2.0,\n",
       " 61: 2.0,\n",
       " 30: 1.6989700043360187,\n",
       " 62: 1.5,\n",
       " 63: 2.0,\n",
       " 64: 2.0,\n",
       " 65: 1.5,\n",
       " 66: 2.0,\n",
       " 67: 1.5,\n",
       " 68: 2.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_on_query(data, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(data, query): # Query . doc / ||Query|| * ||Document||\n",
    "    \n",
    "    query_use = [query]\n",
    "    unique_words = unique_terms(data + query_use)\n",
    "    tfidf_query = tfidf_on_query(data, query_use)\n",
    "    tfidf_data = tfidf(data, query_use)\n",
    "\n",
    "    dot_val = 0\n",
    "    query_val = 0\n",
    "    document_val = 0\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    similarity = []\n",
    "    doc_similarity = {}\n",
    "    \n",
    "    for document in data:\n",
    "        \n",
    "        dot_val = 0\n",
    "        query_val = 0\n",
    "        document_val = 0\n",
    "        counter += 1  #increment \n",
    "        for term in query:\n",
    "            \n",
    "            counter1 = counter - 1\n",
    "            \n",
    "            if term in document:\n",
    "                \n",
    "                term_idx = np.where(np.asarray(unique_words) == term)\n",
    "                term_idx = list(term_idx)\n",
    "                term_idx = int(term_idx[0])\n",
    "                dot_val += tfidf_query[term_idx] * tfidf_data[(counter1, term_idx)]\n",
    "                query_val += tfidf_query[term_idx] * tfidf_query[term_idx]\n",
    "                document_val += tfidf_data[(counter1, term_idx)] * tfidf_data[(counter1, term_idx)]\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                dot_val += 0\n",
    "                query_val += 0\n",
    "                document_val += 0\n",
    "    \n",
    "        query_val = np.sqrt(query_val)\n",
    "        document_val = np.sqrt(document_val)\n",
    "        dot_prod = dot_val / (query_val * document_val)\n",
    "        \n",
    "        doc_similarity[counter1] = dot_prod\n",
    "        \n",
    "    #counter = 0\n",
    "    #top_docs = {}\n",
    "    #for similarity_val in similarity:\n",
    "        \n",
    "        \n",
    "        #counter += 1\n",
    "    \n",
    "    for entry in doc_similarity:\n",
    "        \n",
    "        if np.isnan(doc_similarity[entry]) == True:\n",
    "            \n",
    "            doc_similarity[entry] = 0\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            continue\n",
    "            \n",
    "    top_3_doc_similarity = sorted(doc_similarity, key=doc_similarity.get, reverse=True)[:3]\n",
    "    \n",
    "    first_doc = 'first place: doc' + str(top_3_doc_similarity[0]) + ': ' + ' '.join(data[top_3_doc_similarity[0]])\n",
    "    second_doc = 'second place: doc' + str(top_3_doc_similarity[1]) + ': ' + ' '.join(data[top_3_doc_similarity[1]]) \n",
    "    third_doc = 'third place: doc' + str(top_3_doc_similarity[2]) + ': ' + ' '.join(data[top_3_doc_similarity[2]])\n",
    "    \n",
    "    print(first_doc)\n",
    "    print(second_doc)\n",
    "    print(third_doc)\n",
    "    \n",
    "    first_doc = data[top_3_doc_similarity[0]]\n",
    "    second_doc = data[top_3_doc_similarity[1]]\n",
    "    third_doc = data[top_3_doc_similarity[2]]\n",
    "    print(first_doc)\n",
    "    print(second_doc)\n",
    "    print(third_doc)\n",
    "    #print(data[first_doc])\n",
    "    return first_doc, second_doc, third_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first place: doc2: There are some holy trees that are worshiped by the people.\n",
      "second place: doc8: We all should plant trees near our house.\n",
      "third place: doc0: Trees protect us from pollution.\n",
      "['There', 'are', 'some', 'holy', 'trees', 'that', 'are', 'worshiped', 'by', 'the', 'people.']\n",
      "['We', 'all', 'should', 'plant', 'trees', 'near', 'our', 'house.']\n",
      "['Trees', 'protect', 'us', 'from', 'pollution.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sj/v02vk_y142q73v1g1c6tn_q80000gn/T/ipykernel_5098/4244842324.py:44: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dot_prod = dot_val / (query_val * document_val)\n"
     ]
    }
   ],
   "source": [
    "first_doc, second_doc, third_doc = cosine_similarity(data, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "def Answer(query, document):\n",
    "    #Model\n",
    "    model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad', return_dict = False)\n",
    "    #Tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "    encoding = tokenizer.encode_plus(text = query,text_pair = document, add_special=True)\n",
    "\n",
    "    inputs = encoding['input_ids']  #Token embeddings\n",
    "    #outputs = model(**inputs, return_dict=False)\n",
    "    sentence_embedding = encoding['token_type_ids']  #Segment embeddings\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens\n",
    "\n",
    "    start_scores, end_scores = model(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([sentence_embedding]))\n",
    "\n",
    "    start_index = torch.argmax(start_scores)\n",
    "\n",
    "    end_index = torch.argmax(end_scores)\n",
    "\n",
    "    answer = ' '.join(tokens[start_index:end_index+1])\n",
    "\n",
    "    corrected_answer = ''\n",
    "\n",
    "    for word in answer.split():\n",
    "\n",
    "        #If it's a subword token\n",
    "        if word[0:2] == '##':\n",
    "            corrected_answer += word[2:]\n",
    "        else:\n",
    "            corrected_answer += ' ' + word\n",
    "\n",
    "    return corrected_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'add_special': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " holy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'add_special': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " plant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'add_special': True} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " protect\n"
     ]
    }
   ],
   "source": [
    "query = \"What do trees do?\"\n",
    "print(Answer(query, first_doc))\n",
    "print(Answer(query, second_doc))\n",
    "print(Answer(query, third_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
