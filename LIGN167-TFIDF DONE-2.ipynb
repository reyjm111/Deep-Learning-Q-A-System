{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'trees', 'look', 'very', 'very', 'very', 'green'],\n",
       " ['The', 'grass', 'is', 'very', 'very', 'green']]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_1 = 'The trees look very very very green'\n",
    "doc_1 = doc_1.split()\n",
    "doc_2 = 'The grass is very very green'\n",
    "doc_2 = doc_2.split()\n",
    "data = [doc_1, doc_2]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_term_count(term, document): # count times term appears in the document\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    for word in document:\n",
    "\n",
    "        if term == word:\n",
    "\n",
    "            count = count + 1 \n",
    "\n",
    "        else:\n",
    "            \n",
    "            count = count\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "#document_term_count(data[0][0], data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_d_words(document): # count all words in document\n",
    "\n",
    "    count_words = len(document)\n",
    "\n",
    "    return count_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(term, document): # calculate term frequency ; if low, then not much relevance in document, if high then high relevance in document\n",
    "\n",
    "    tf = document_term_count(term, document) / num_d_words(document)\n",
    "\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "term_frequency(data[1][0], data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_d_words_augment(document): # find the most frequent word and return how many times it occurred\n",
    "\n",
    "    count_all_words = []\n",
    "\n",
    "    for term in document:\n",
    "\n",
    "        count = document.count(term)\n",
    "\n",
    "        count_all_words.append(count)\n",
    "\n",
    "    max_word = max(count_all_words)\n",
    "\n",
    "    return max_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_term_frequency(term, document): # normalization based on comparing our word against the most frequent word in document ; reduces bias towards longer document\n",
    "\n",
    "    return 0.5 + 0.5 * (document_term_count(term, document) / num_d_words_augment(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "augmented_term_frequency(data[0][0], data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_relevant_documents(term, document_set): # count number of documents that contain the term\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    for document in document_set:\n",
    "\n",
    "        if term in document:\n",
    "\n",
    "            count = count + 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            count = count\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_documents(document_set): # count total number of documents in document set\n",
    "\n",
    "    count_docs = len(document_set)\n",
    "\n",
    "    return count_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_document_frequency(term, document_set): # calculate idf ; weighs down common words while scaling up rare words; measures how informative t is; higher idf means rare word, lower idf means less common word\n",
    "\n",
    "    idf = math.log10(num_documents(document_set) / (num_relevant_documents(term, document_set) + 1))\n",
    "\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.17609125905568127\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "print(inverse_document_frequency(data[1][0], data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'trees', 'look', 'very', 'green', 'grass', 'is']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unique_terms(document_set): # gather all unique words in all documents\n",
    "\n",
    "    unique_terms = []\n",
    "\n",
    "    for document in document_set:\n",
    "\n",
    "        for term in document:\n",
    "\n",
    "            if term not in unique_terms:\n",
    "\n",
    "                unique_terms.append(term)\n",
    "            \n",
    "            else:\n",
    "\n",
    "                pass\n",
    "    \n",
    "    return unique_terms\n",
    "\n",
    "#unique_terms(data)\n",
    "unique_words = unique_terms(data)\n",
    "#unique_words = dict.fromkeys(unique_words)\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def tfidf(document_set): # calculate tfidf for all words and return a dictionary with (x, y) : #, where x is the document index, y is the index of the word in unique_words\n",
    "\n",
    "    unique_words = unique_terms(document_set)\n",
    "    \n",
    "    tfidf_vals = {}\n",
    "    counter = 0\n",
    "    for document in document_set:\n",
    "        \n",
    "        unique_terms_list = []\n",
    "        counter = counter + 1\n",
    "        for term in document:\n",
    "            counter1 = counter - 1\n",
    "            if term in unique_terms_list:\n",
    "                \n",
    "                pass\n",
    "            \n",
    "            else:\n",
    "            \n",
    "                tf = augmented_term_frequency(term, document)\n",
    "                idf = inverse_document_frequency(term, document_set)\n",
    "            \n",
    "                tfidf = tf * idf\n",
    "                term_idx = np.where(np.asarray(unique_words) == term)\n",
    "                term_idx = list(term_idx)\n",
    "                term_idx = int(term_idx[0])\n",
    "                tfidf_vals[counter1, term_idx] = tfidf\n",
    "                unique_terms_list.append(term)\n",
    "    \n",
    "    return tfidf_vals  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): -0.1173941727037875,\n",
       " (0, 1): 0.0,\n",
       " (0, 2): 0.0,\n",
       " (0, 3): -0.17609125905568127,\n",
       " (0, 4): -0.1173941727037875,\n",
       " (1, 0): -0.13206844429176096,\n",
       " (1, 5): 0.0,\n",
       " (1, 6): 0.0,\n",
       " (1, 3): -0.17609125905568127,\n",
       " (1, 4): -0.13206844429176096}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn TFIDF Model (how it should look)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.34211869506421816\n",
      "  (0, 0)\t0.34211869506421816\n",
      "  (0, 9)\t0.34211869506421816\n",
      "  (0, 5)\t0.34211869506421816\n",
      "  (0, 11)\t0.34211869506421816\n",
      "  (0, 12)\t0.48684053853849035\n",
      "  (0, 4)\t0.24342026926924518\n",
      "  (0, 10)\t0.24342026926924518\n",
      "  (0, 2)\t0.24342026926924518\n",
      "  (1, 3)\t0.40740123733358447\n",
      "  (1, 6)\t0.40740123733358447\n",
      "  (1, 7)\t0.40740123733358447\n",
      "  (1, 8)\t0.40740123733358447\n",
      "  (1, 12)\t0.28986933576883284\n",
      "  (1, 4)\t0.28986933576883284\n",
      "  (1, 10)\t0.28986933576883284\n",
      "  (1, 2)\t0.28986933576883284\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#for the sentence, make sure all words are lowercase or you will run #into error. for simplicity, I just made the same sentence all #lowercase\n",
    "firstV= \"Data Science is the sexiest job of the 21st century\"\n",
    "secondV= \"machine learning is the key for data science\"\n",
    "#calling the TfidfVectorizer\n",
    "vectorize= TfidfVectorizer()\n",
    "#fitting the model and passing our sentences right away:\n",
    "response= vectorize.fit_transform([firstV, secondV])\n",
    "print(response)\n",
    "#(0,1)   0.342118\n",
    "# 0 = DOCUMENT INDEX\n",
    "# 1 = UNIQUE WORD DICTIONARY INDEX\n",
    "# 0.342118 = TFIDF SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wiki_bio (/Users/reymendoza/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "wiki_bio_dataset = load_dataset('wiki_bio', split='test[0:100]')\n",
    "docs = wiki_bio_dataset['target_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who plays bass for Death From Above?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 747)\t0.5314425144796117\n",
      "  (0, 405)\t0.701772782906254\n",
      "  (0, 1779)\t0.47442998954071103\n",
      "  (1, 1688)\t0.19222225030393428\n",
      "  (1, 2296)\t0.20944125331450172\n",
      "  (1, 834)\t0.19222225030393428\n",
      "  (1, 107)\t0.15623980910960442\n",
      "  (1, 1316)\t0.13335009015917476\n",
      "  (1, 1748)\t0.20944125331450172\n",
      "  (1, 2074)\t0.19222225030393428\n",
      "  (1, 2089)\t0.19222225030393428\n",
      "  (1, 2444)\t0.19222225030393428\n",
      "  (1, 1759)\t0.20944125331450172\n",
      "  (1, 2019)\t0.16278617023155453\n",
      "  (1, 1776)\t0.12387378898052172\n",
      "  (1, 402)\t0.41888250662900345\n",
      "  (1, 1373)\t0.2536074580744494\n",
      "  (1, 1454)\t0.38444450060786856\n",
      "  (1, 2023)\t0.04922233274126\n",
      "  (1, 92)\t0.18000517324212198\n",
      "  (1, 11)\t0.16278617023155453\n",
      "  (1, 973)\t0.11613108714860731\n",
      "  (1, 464)\t0.05667316203770663\n",
      "  (1, 1432)\t0.04922233274126\n",
      "  (1, 1891)\t0.20944125331450172\n",
      "  :\t:\n",
      "  (100, 563)\t0.30776906778520163\n",
      "  (100, 2323)\t0.09606939237895243\n",
      "  (100, 2001)\t0.30776906778520163\n",
      "  (100, 1228)\t0.19213878475790486\n",
      "  (100, 104)\t0.10258968926173388\n",
      "  (100, 211)\t0.10258968926173388\n",
      "  (100, 109)\t0.09101185719906839\n",
      "  (100, 87)\t0.09606939237895243\n",
      "  (100, 1411)\t0.10258968926173388\n",
      "  (100, 158)\t0.09101185719906839\n",
      "  (100, 2506)\t0.08035925318885291\n",
      "  (100, 437)\t0.07530171800896888\n",
      "  (100, 1275)\t0.09101185719906839\n",
      "  (100, 1497)\t0.36404742879627355\n",
      "  (100, 1965)\t0.15537941552863263\n",
      "  (100, 2494)\t0.2774209908903291\n",
      "  (100, 2096)\t0.07530171800896888\n",
      "  (100, 1843)\t0.06767559498751602\n",
      "  (100, 2475)\t0.08035925318885291\n",
      "  (100, 2486)\t0.14233882176306972\n",
      "  (100, 2288)\t0.12655012117749798\n",
      "  (100, 107)\t0.08338573417761552\n",
      "  (100, 2023)\t0.02627013164542159\n",
      "  (100, 464)\t0.030246665376850067\n",
      "  (100, 1432)\t0.02627013164542159\n",
      "\n",
      "  (0, 747)\t0.5314425144796117\n",
      "  (0, 405)\t0.701772782906254\n",
      "  (0, 1779)\t0.47442998954071103\n",
      "\n",
      "  (0, 1688)\t0.19222225030393428\n",
      "  (0, 2296)\t0.20944125331450172\n",
      "  (0, 834)\t0.19222225030393428\n",
      "  (0, 107)\t0.15623980910960442\n",
      "  (0, 1316)\t0.13335009015917476\n",
      "  (0, 1748)\t0.20944125331450172\n",
      "  (0, 2074)\t0.19222225030393428\n",
      "  (0, 2089)\t0.19222225030393428\n",
      "  (0, 2444)\t0.19222225030393428\n",
      "  (0, 1759)\t0.20944125331450172\n",
      "  (0, 2019)\t0.16278617023155453\n",
      "  (0, 1776)\t0.12387378898052172\n",
      "  (0, 402)\t0.41888250662900345\n",
      "  (0, 1373)\t0.2536074580744494\n",
      "  (0, 1454)\t0.38444450060786856\n",
      "  (0, 2023)\t0.04922233274126\n",
      "  (0, 92)\t0.18000517324212198\n",
      "  (0, 11)\t0.16278617023155453\n",
      "  (0, 973)\t0.11613108714860731\n",
      "  (0, 464)\t0.05667316203770663\n",
      "  (0, 1432)\t0.04922233274126\n",
      "  (0, 1891)\t0.20944125331450172\n",
      "  (0, 2114)\t0.20944125331450172\n",
      "  (0, 1387)\t0.20944125331450172\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "# Initialize a vectorizer that removes English stop words\n",
    "vectorizer = TfidfVectorizer(analyzer=\"word\", stop_words='english')\n",
    "\n",
    "# Create a corpus of query and documents and convert to TFIDF vectors\n",
    "query_and_docs = [query] + docs\n",
    "matrix = vectorizer.fit_transform(query_and_docs)\n",
    "print(matrix)\n",
    "print( )\n",
    "print(matrix[0]) # matrix[0], the 0 indexes which document which in this case is 0\n",
    "print( )\n",
    "print(matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
